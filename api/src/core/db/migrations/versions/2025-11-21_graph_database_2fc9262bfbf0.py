# type: ignore
"""graph database

Revision ID: 2fc9262bfbf0
Revises: 49d7cafafc27
Create Date: 2025-11-21 06:40:16.671648+00:00

"""

from __future__ import annotations

import warnings
from typing import TYPE_CHECKING

import sqlalchemy as sa
from alembic import op
from advanced_alchemy.types import (
    EncryptedString,
    EncryptedText,
    GUID,
    ORA_JSONB,
    DateTimeUTC,
)
from sqlalchemy import Text  # noqa: F401
import advanced_alchemy.types
import advanced_alchemy.types.datetime
import advanced_alchemy.types.json
from sqlalchemy.dialects import postgresql

if TYPE_CHECKING:
    pass

__all__ = [
    "downgrade",
    "upgrade",
    "schema_upgrades",
    "schema_downgrades",
    "data_upgrades",
    "data_downgrades",
]

sa.GUID = GUID
sa.DateTimeUTC = DateTimeUTC
sa.ORA_JSONB = ORA_JSONB
sa.EncryptedString = EncryptedString
sa.EncryptedText = EncryptedText
# Additional type aliases for proper migration generation
sa.Text = Text


# revision identifiers, used by Alembic.
revision = "2fc9262bfbf0"
down_revision = "49d7cafafc27"
branch_labels = None
depends_on = None


def upgrade() -> None:
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=UserWarning)
        with op.get_context().autocommit_block():
            schema_upgrades()
            data_upgrades()


def downgrade() -> None:
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=UserWarning)
        with op.get_context().autocommit_block():
            data_downgrades()
            schema_downgrades()


def schema_upgrades() -> None:
    """schema upgrade migrations go here."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "knowledge_graphs",
        sa.Column("name", sa.String(length=255), nullable=False, comment="Entity name"),
        sa.Column("description", sa.Text, nullable=True, comment="Entity description"),
        sa.Column(
            "system_name",
            sa.String(length=255),
            nullable=False,
            comment="System name of the entity",
        ),
        sa.Column(
            "category", sa.String(length=255), nullable=True, comment="Entity category"
        ),
        sa.Column(
            "created_by",
            sa.String(length=36),
            nullable=True,
            comment="ID of the user who created the entity",
        ),
        sa.Column(
            "updated_by",
            sa.String(length=36),
            nullable=True,
            comment="ID of the user who last updated the entity",
        ),
        sa.Column("id", GUID, nullable=False),
        sa.Column("sa_orm_sentinel", sa.Integer(), nullable=True),
        sa.Column(
            "created_at",
            advanced_alchemy.types.datetime.DateTimeUTC(timezone=True),
            nullable=False,
        ),
        sa.Column(
            "updated_at",
            advanced_alchemy.types.datetime.DateTimeUTC(timezone=True),
            nullable=False,
        ),
        sa.Column(
            "settings",
            postgresql.JSONB(astext_type=sa.Text()),
            nullable=True,
            comment="Knowledge graph settings (processing config, etc.)",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_knowledge_graphs")),
        sa.UniqueConstraint(
            "system_name", name=op.f("uq_knowledge_graphs_system_name")
        ),
    )
    op.create_index(
        op.f("ix_knowledge_graphs_description"),
        "knowledge_graphs",
        ["description"],
        unique=False,
    )
    op.create_index(
        op.f("ix_knowledge_graphs_name"), "knowledge_graphs", ["name"], unique=False
    )

    # Create knowledge_graph_sources table
    op.create_table(
        "knowledge_graph_sources",
        sa.Column("id", sa.UUID(), nullable=False),
        sa.Column("name", sa.String(length=255), nullable=False, comment="Source name"),
        sa.Column("created_at", sa.DateTime(timezone=True), nullable=False),
        sa.Column("updated_at", sa.DateTime(timezone=True), nullable=False),
        sa.Column(
            "graph_id",
            sa.UUID(),
            nullable=False,
            comment="Foreign key to knowledge_graphs",
        ),
        sa.Column(
            "type",
            sa.String(length=100),
            nullable=False,
            comment="Source type (e.g., 'sharepoint', 'confluence', 'upload')",
        ),
        sa.Column(
            "config",
            postgresql.JSONB(astext_type=sa.Text()),
            nullable=True,
            comment="Source configuration (connection details, credentials, etc.)",
        ),
        sa.Column(
            "status", sa.String(length=50), nullable=True, comment="Source status"
        ),
        sa.Column(
            "documents_count",
            sa.Integer(),
            nullable=True,
            comment="Number of documents from this source",
        ),
        sa.Column(
            "last_sync_at",
            sa.String(length=100),
            nullable=True,
            comment="Last sync timestamp",
        ),
        sa.Column("sa_orm_sentinel", sa.Integer(), nullable=True),
        sa.ForeignKeyConstraint(
            ["graph_id"], ["knowledge_graphs.id"], ondelete="CASCADE"
        ),
        sa.PrimaryKeyConstraint("id"),
    )

    # Create indexes for knowledge_graph_sources
    op.create_index(
        op.f("ix_knowledge_graph_sources_graph_id"),
        "knowledge_graph_sources",
        ["graph_id"],
        unique=False,
    )
    op.create_index(
        op.f("ix_knowledge_graph_sources_type"),
        "knowledge_graph_sources",
        ["type"],
        unique=False,
    )

    op.create_table(
        "knowledge_graph_documents",
        sa.Column(
            "graph_id", GUID, nullable=True, comment="Foreign key to knowledge_graphs"
        ),
        sa.Column(
            "source_id",
            GUID,
            nullable=True,
            comment="Foreign key to knowledge_graph_sources",
        ),
        sa.Column(
            "type",
            sa.String(length=100),
            nullable=True,
            comment="Document type (e.g., 'pdf', 'doc')",
        ),
        sa.Column(
            "title", sa.String(length=500), nullable=True, comment="Document title"
        ),
        sa.Column("summary", sa.Text, nullable=True, comment="Document summary"),
        sa.Column(
            "toc",
            sa.JSON()
            .with_variant(postgresql.JSONB(astext_type=sa.Text), "cockroachdb")
            .with_variant(advanced_alchemy.types.json.ORA_JSONB(), "oracle")
            .with_variant(postgresql.JSONB(astext_type=sa.Text), "postgresql"),
            nullable=True,
            comment="Document table of contents",
        ),
        sa.Column(
            "embedding",
            sa.JSON()
            .with_variant(postgresql.JSONB(astext_type=sa.Text), "cockroachdb")
            .with_variant(advanced_alchemy.types.json.ORA_JSONB(), "oracle")
            .with_variant(postgresql.JSONB(astext_type=sa.Text), "postgresql"),
            nullable=True,
            comment="Embedding vector for the document",
        ),
        sa.Column(
            "status",
            sa.String(length=50),
            nullable=True,
            comment="Processing status (pending, processing, completed, error)",
        ),
        sa.Column(
            "status_message",
            sa.Text,
            nullable=True,
            comment="Processing status message",
        ),
        sa.Column(
            "total_pages",
            sa.Integer(),
            nullable=True,
            comment="Total number of pages in the document",
        ),
        sa.Column(
            "content_profile",
            sa.String(length=100),
            nullable=True,
            comment="Content profile name used for ingestion",
        ),
        sa.Column(
            "processing_time",
            sa.Float(),
            nullable=True,
            comment="Processing time in seconds",
        ),
        sa.Column("name", sa.String(length=255), nullable=False, comment="Source name"),
        sa.Column("id", GUID, nullable=False),
        sa.Column("sa_orm_sentinel", sa.Integer(), nullable=True),
        sa.Column(
            "created_at",
            advanced_alchemy.types.datetime.DateTimeUTC(timezone=True),
            nullable=False,
        ),
        sa.Column(
            "updated_at",
            advanced_alchemy.types.datetime.DateTimeUTC(timezone=True),
            nullable=False,
        ),
        sa.ForeignKeyConstraint(
            ["graph_id"],
            ["knowledge_graphs.id"],
            name=op.f("fk_knowledge_graph_documents_graph_id_knowledge_graphs"),
            ondelete="SET NULL",
        ),
        sa.ForeignKeyConstraint(
            ["source_id"],
            ["knowledge_graph_sources.id"],
            name=op.f("fk_knowledge_graph_documents_source_id_knowledge_graph_sources"),
            ondelete="SET NULL",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_knowledge_graph_documents")),
    )
    op.create_table(
        "knowledge_graph_chunks",
        sa.Column(
            "generated_id",
            sa.String(length=1000),
            nullable=True,
            comment="Generated ID from AI chunking process",
        ),
        sa.Column("title", sa.String(length=500), nullable=True, comment="Chunk title"),
        sa.Column(
            "toc_reference",
            sa.String(length=500),
            nullable=True,
            comment="TOC reference",
        ),
        # sa.Column('parent', sa.String(length=500), nullable=True, comment='Parent chunk reference'),
        sa.Column(
            "index", sa.Integer(), nullable=True, comment="Chunk index in the document"
        ),
        sa.Column(
            "page",
            sa.Integer(),
            nullable=True,
            comment="Page number where the chunk starts",
        ),
        sa.Column("text", sa.Text, nullable=True, comment="Chunk text content"),
        sa.Column(
            "embedding",
            sa.JSON()
            .with_variant(postgresql.JSONB(astext_type=sa.Text), "cockroachdb")
            .with_variant(advanced_alchemy.types.json.ORA_JSONB(), "oracle")
            .with_variant(postgresql.JSONB(astext_type=sa.Text), "postgresql"),
            nullable=True,
            comment="Embedding vector for the chunk",
        ),
        sa.Column(
            "chunk_type",
            sa.String(length=50),
            nullable=True,
            comment="Chunk type (TEXT, TABLE, etc.)",
        ),
        sa.Column(
            "document_id",
            GUID,
            nullable=True,
            comment="Foreign key to knowledge_graph_documents",
        ),
        sa.Column("name", sa.String(length=255), nullable=False, comment="Source name"),
        sa.Column("id", GUID, nullable=False),
        sa.Column("sa_orm_sentinel", sa.Integer(), nullable=True),
        sa.Column(
            "created_at",
            advanced_alchemy.types.datetime.DateTimeUTC(timezone=True),
            nullable=False,
        ),
        sa.Column(
            "updated_at",
            advanced_alchemy.types.datetime.DateTimeUTC(timezone=True),
            nullable=False,
        ),
        sa.ForeignKeyConstraint(
            ["document_id"],
            ["knowledge_graph_documents.id"],
            name=op.f(
                "fk_knowledge_graph_chunks_document_id_knowledge_graph_documents"
            ),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_knowledge_graph_chunks")),
    )
    # ### end Alembic commands ###


def schema_downgrades() -> None:
    """schema downgrade migrations go here."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index(
        op.f("ix_knowledge_graph_chunks_name"), table_name="knowledge_graph_chunks"
    )
    op.drop_index(
        op.f("ix_knowledge_graph_chunks_description"),
        table_name="knowledge_graph_chunks",
    )
    op.drop_table("knowledge_graph_chunks")
    op.drop_index(
        op.f("ix_knowledge_graph_documents_name"),
        table_name="knowledge_graph_documents",
    )
    op.drop_index(
        op.f("ix_knowledge_graph_documents_description"),
        table_name="knowledge_graph_documents",
    )
    op.drop_table("knowledge_graph_documents")

    # Drop knowledge_graph_sources and indexes
    op.drop_index(
        op.f("ix_knowledge_graph_sources_type"), table_name="knowledge_graph_sources"
    )
    op.drop_index(
        op.f("ix_knowledge_graph_sources_graph_id"),
        table_name="knowledge_graph_sources",
    )
    op.drop_table("knowledge_graph_sources")

    op.drop_index(op.f("ix_knowledge_graphs_name"), table_name="knowledge_graphs")
    op.drop_index(
        op.f("ix_knowledge_graphs_description"), table_name="knowledge_graphs"
    )
    op.drop_table("knowledge_graphs")
    # ### end Alembic commands ###


def data_upgrades() -> None:
    """Add any optional data upgrade migrations here!"""


def data_downgrades() -> None:
    """Add any optional data downgrade migrations here!"""

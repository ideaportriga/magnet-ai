# type: ignore
"""add tables for deep research

Revision ID: 8eaf9d88a99f
Revises: 1c3d5e7f9a0b
Create Date: 2025-12-18 16:01:22.017379+00:00

"""

from __future__ import annotations

import warnings
from typing import TYPE_CHECKING

import sqlalchemy as sa
from alembic import op
from advanced_alchemy.types import (
    EncryptedString,
    EncryptedText,
    GUID,
    ORA_JSONB,
    DateTimeUTC,
)
from sqlalchemy import Text  # noqa: F401
import advanced_alchemy.types
import advanced_alchemy.types.datetime
import advanced_alchemy.types.json
from sqlalchemy.dialects import postgresql

if TYPE_CHECKING:
    pass

__all__ = [
    "downgrade",
    "upgrade",
    "schema_upgrades",
    "schema_downgrades",
    "data_upgrades",
    "data_downgrades",
]

sa.GUID = GUID
sa.DateTimeUTC = DateTimeUTC
sa.ORA_JSONB = ORA_JSONB
sa.EncryptedString = EncryptedString
sa.EncryptedText = EncryptedText
# Additional type aliases for proper migration generation
sa.Text = Text


# revision identifiers, used by Alembic.
revision = "8eaf9d88a99f"
down_revision = "1c3d5e7f9a0b"
branch_labels = None
depends_on = None


def upgrade() -> None:
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=UserWarning)
        with op.get_context().autocommit_block():
            schema_upgrades()
            data_upgrades()


def downgrade() -> None:
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=UserWarning)
        with op.get_context().autocommit_block():
            data_downgrades()
            schema_downgrades()


def schema_upgrades() -> None:
    """schema upgrade migrations go here."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "deep_research_configs",
        sa.Column("id", GUID, nullable=False),
        sa.Column(
            "config",
            sa.JSON()
            .with_variant(postgresql.JSONB(astext_type=sa.Text), "cockroachdb")
            .with_variant(advanced_alchemy.types.json.ORA_JSONB(), "oracle")
            .with_variant(postgresql.JSONB(astext_type=sa.Text), "postgresql"),
            nullable=True,
            comment="Deep research configuration in JSON format",
        ),
        sa.Column("name", sa.String(length=255), nullable=False, comment="Entity name"),
        sa.Column("description", sa.Text, nullable=True, comment="Entity description"),
        sa.Column(
            "system_name",
            sa.String(length=255),
            nullable=False,
            comment="System name of the entity",
        ),
        sa.Column(
            "category", sa.String(length=255), nullable=True, comment="Entity category"
        ),
        sa.Column(
            "created_by",
            sa.String(length=36),
            nullable=True,
            comment="ID of the user who created the entity",
        ),
        sa.Column(
            "updated_by",
            sa.String(length=36),
            nullable=True,
            comment="ID of the user who last updated the entity",
        ),
        sa.Column("sa_orm_sentinel", sa.Integer(), nullable=True),
        sa.Column(
            "created_at",
            advanced_alchemy.types.datetime.DateTimeUTC(timezone=True),
            nullable=False,
        ),
        sa.Column(
            "updated_at",
            advanced_alchemy.types.datetime.DateTimeUTC(timezone=True),
            nullable=False,
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_deep_research_configs")),
        sa.UniqueConstraint(
            "system_name", name=op.f("uq_deep_research_configs_system_name")
        ),
    )
    op.create_index(
        op.f("ix_deep_research_configs_description"),
        "deep_research_configs",
        ["description"],
        unique=False,
    )
    op.create_index(
        op.f("ix_deep_research_configs_name"),
        "deep_research_configs",
        ["name"],
        unique=False,
    )
    op.create_table(
        "deep_research_runs",
        sa.Column("id", GUID, nullable=False),
        sa.Column(
            "client_id",
            sa.String(length=255),
            nullable=True,
            comment="Optional client-side identifier",
        ),
        sa.Column(
            "status",
            sa.String(length=50),
            nullable=False,
            comment="Run status: pending, running, completed, failed",
        ),
        sa.Column(
            "input",
            sa.JSON()
            .with_variant(postgresql.JSONB(astext_type=sa.Text), "cockroachdb")
            .with_variant(advanced_alchemy.types.json.ORA_JSONB(), "oracle")
            .with_variant(postgresql.JSONB(astext_type=sa.Text), "postgresql"),
            nullable=True,
            comment="Input data for the research (e.g., {'query': 'research question'})",
        ),
        sa.Column(
            "config",
            sa.JSON()
            .with_variant(postgresql.JSONB(astext_type=sa.Text), "cockroachdb")
            .with_variant(advanced_alchemy.types.json.ORA_JSONB(), "oracle")
            .with_variant(postgresql.JSONB(astext_type=sa.Text), "postgresql"),
            nullable=True,
            comment="Configuration snapshot used for this run",
        ),
        sa.Column(
            "config_system_name",
            sa.String(length=255),
            nullable=True,
            comment="System name of the config used for this run",
        ),
        sa.Column(
            "details",
            sa.JSON()
            .with_variant(postgresql.JSONB(astext_type=sa.Text), "cockroachdb")
            .with_variant(advanced_alchemy.types.json.ORA_JSONB(), "oracle")
            .with_variant(postgresql.JSONB(astext_type=sa.Text), "postgresql"),
            nullable=True,
            comment="Run execution details in JSON format (progress, memory, iterations, result, error)",
        ),
        sa.Column("sa_orm_sentinel", sa.Integer(), nullable=True),
        sa.Column(
            "created_at",
            advanced_alchemy.types.datetime.DateTimeUTC(timezone=True),
            nullable=False,
        ),
        sa.Column(
            "updated_at",
            advanced_alchemy.types.datetime.DateTimeUTC(timezone=True),
            nullable=False,
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("pk_deep_research_runs")),
    )
    op.create_index(
        op.f("ix_deep_research_runs_client_id"),
        "deep_research_runs",
        ["client_id"],
        unique=False,
    )
    op.create_index(
        "ix_deep_research_runs_config_system_name",
        "deep_research_runs",
        ["config_system_name"],
    )
    op.create_index(
        op.f("ix_deep_research_runs_status"),
        "deep_research_runs",
        ["status"],
        unique=False,
    )
    # ### end Alembic commands ###


def schema_downgrades() -> None:
    """schema downgrade migrations go here."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index(op.f("ix_deep_research_runs_status"), table_name="deep_research_runs")
    op.drop_index(
        "ix_deep_research_runs_config_system_name", table_name="deep_research_runs"
    )
    op.drop_index(
        op.f("ix_deep_research_runs_client_id"), table_name="deep_research_runs"
    )
    op.drop_table("deep_research_runs")
    op.drop_index(
        op.f("ix_deep_research_configs_name"), table_name="deep_research_configs"
    )
    op.drop_index(
        op.f("ix_deep_research_configs_description"), table_name="deep_research_configs"
    )
    op.drop_table("deep_research_configs")
    # ### end Alembic commands ###


def data_upgrades() -> None:
    """Add any optional data upgrade migrations here!"""


def data_downgrades() -> None:
    """Add any optional data downgrade migrations here!"""
